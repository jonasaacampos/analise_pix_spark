{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fpXPtmLKKxnk"
   },
   "source": [
    "## 1 - Preparação do Ambiente de Desenvolvimento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Preparação do ambiente</summary>\n",
    "\n",
    "### Preparação do ambiente\n",
    "\n",
    "#### IDEs utilizada\n",
    "\n",
    " - VSCode\n",
    "\n",
    "#### Criar ambiente virtual\n",
    "\n",
    "- Command Pallet (ctrl + shift+ p)\n",
    "- Python: Create Environment > Venv > Python Version (3.12)'\n",
    "\n",
    "#### Ativar .venv\n",
    "\n",
    "In VsCode terminal, alterar a política de execução de scripts para ativar o ambiente virtual.\n",
    "\n",
    "```bash\n",
    "Set-ExecutionPolicy Unrestricted -Scope Process\n",
    "\n",
    "# ativar ambiente virtual\n",
    ".\\.venv\\Scripts\\activate\n",
    "\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFBKW-UUAMOo"
   },
   "source": [
    "## 2 - Data Undesrtanting\n",
    "\n",
    "Primeiramente, devemos entender tudo sobre a fonte dos dados\n",
    "- Como o dado chega até nós?\n",
    "- Qual formato virá? \n",
    "- Aonde o processamento será executado (AWS EMR, Cluster On-Premise)? \n",
    "- De quanto em quanto tempo eu preciso gerar esse relatório (mensal, diário, near-real time)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os dados foram compartilhados via `*.json`. Saber como os dados serão ingeridos são de vital importância para delimitar a forma como lidaremos com nosso projeto. Análises em tempo real (streaming) são diferentes de análises em lotes (bacthes). Análises pontuais como esta também adotam uma estratégia diferentes das que requerem análises periódicas.\n",
    "\n",
    "### Data Schema\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id_transacao\": inteiro,\n",
    "  \"valor\": texto,\n",
    "  \"remetente\": {\n",
    "      \"nome\": texto,\n",
    "      \"banco\": texto,\n",
    "      \"tipo\": texto\n",
    "  }, \n",
    "  \"destinatario\": {\n",
    "      \"nome\": texto, \n",
    "      \"banco\":texto,\n",
    "      \"tipo\": texto\n",
    "  },        \n",
    "  \"categoria\": texto,\n",
    "  \"transaction_date\":texto,\n",
    "  \"chave_pix\":texto,\n",
    "  \"fraude\":inteiro,\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KPvcwRICAPod"
   },
   "source": [
    "## 3 - Preparação dos Dados\n",
    "\n",
    "Agora é hora de começar a preparar os dados de acordo com as necessidades do escopo de trabalho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.7 (from pyspark)\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=5bc88419a137357dea2effa4ec5826623d64b048159718d468a21ce1bda560ca\n",
      "  Stored in directory: /home/codespace/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funções para inicializar o spark\n",
    "\n",
    "def spark_initialize_session(app_name = 'My Analysis'):\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .config('spark.ui.port', '4050')\n",
    "        .appName(app_name)\n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "    return spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path = f'data{os.sep}case_final.json'\n",
    "spark = spark_initialize_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**não usar o json formatado, isso causa lentidão e erros no algoritmo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>json schema anotations</summary>\n",
    "\n",
    "### Data Schema\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id_transacao\": inteiro,\n",
    "  \"valor\": texto,\n",
    "  \"remetente\": {\n",
    "      \"nome\": texto,\n",
    "      \"banco\": texto,\n",
    "      \"tipo\": texto\n",
    "  }, \n",
    "  \"destinatario\": {\n",
    "      \"nome\": texto, \n",
    "      \"banco\":texto,\n",
    "      \"tipo\": texto\n",
    "  },        \n",
    "  \"categoria\": texto,\n",
    "  \"transaction_date\":texto,\n",
    "  \"chave_pix\":texto,\n",
    "  \"fraude\":inteiro,\n",
    "}\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definição do data schema\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "\n",
    "# amostra dos dados\n",
    "# { \"id_transacao\": 100999, \"valor\": 7058.09, \"remetente\": { \"nome\": \"Jonathan Gonsalves\", \"banco\": \"BTG\", \"tipo\": \"PF\" }, \"destinatario\": { \"nome\": \"Lais Nascimento\", \"banco\": \"Nubank\", \"tipo\": \"PF\" }, \"chave_pix\": \"aleatoria\", \"categoria\": \"vestuario\", \"transaction_date\": \"2022-02-25 09:31:47\", \"fraude\": 0 }\n",
    "\n",
    "data_schema_pix_remetente_destinatario = StructType([\n",
    "    StructField('nome', StringType()),\n",
    "    StructField('banco', StringType()),\n",
    "    StructField('tipo', StringType())\n",
    "    ])\n",
    "\n",
    "data_schema_pix = StructType([\n",
    "    StructField('id_transacao', IntegerType()),\n",
    "    StructField('valor', DoubleType()),\n",
    "    StructField('remetente', data_schema_pix_remetente_destinatario),   \n",
    "    StructField('destinatario', data_schema_pix_remetente_destinatario),\n",
    "    StructField('categoria', StringType()),\n",
    "    StructField('transaction_date', StringType()),\n",
    "    StructField('chave_pix', StringType(), True),\n",
    "    StructField('fraude', IntegerType(), True),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path = f'data{os.sep}case_final.json'\n",
    "spark = spark_initialize_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"transaction_date\": \"2022-02-25 09:31:47\"\n",
    "df = spark.read.json(df_path, \n",
    "                     schema=data_schema_pix, \n",
    "                     timestampFormat='yyyy-MM-dd HH:mm:ss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_transacao: integer (nullable = true)\n",
      " |-- valor: double (nullable = true)\n",
      " |-- remetente: struct (nullable = true)\n",
      " |    |-- nome: string (nullable = true)\n",
      " |    |-- banco: string (nullable = true)\n",
      " |    |-- tipo: string (nullable = true)\n",
      " |-- destinatario: struct (nullable = true)\n",
      " |    |-- nome: string (nullable = true)\n",
      " |    |-- banco: string (nullable = true)\n",
      " |    |-- tipo: string (nullable = true)\n",
      " |-- categoria: string (nullable = true)\n",
      " |-- transaction_date: string (nullable = true)\n",
      " |-- chave_pix: string (nullable = true)\n",
      " |-- fraude: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# verificar tipo dos dados\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+--------------------+--------------------+-------------+-------------------+---------+------+\n",
      "|id_transacao|             valor|           remetente|        destinatario|    categoria|   transaction_date|chave_pix|fraude|\n",
      "+------------+------------------+--------------------+--------------------+-------------+-------------------+---------+------+\n",
      "|        1000|            588.08|{Jonathan Gonsalv...|{Calebe Melo, Cai...|       outros|2021-07-16 05:00:55|aleatoria|     0|\n",
      "|        1001|           80682.5|{Jonathan Gonsalv...|{Davi Lucas Perei...|transferencia|2022-04-20 12:34:01|  celular|     1|\n",
      "|        1002|             549.9|{Jonathan Gonsalv...|{Sabrina Castro, ...|        lazer|2022-07-10 16:51:34|      cpf|     0|\n",
      "|        1003|             90.83|{Jonathan Gonsalv...|{Francisco da Con...|   transporte|2022-10-20 10:57:36|aleatoria|     0|\n",
      "|        1004|13272.619999999999|{Jonathan Gonsalv...|{Isabelly Ferreir...|transferencia|2021-04-06 20:26:51|    email|     0|\n",
      "+------------+------------------+--------------------+--------------------+-------------+-------------------+---------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# visualiar os dados no dataframe\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*precisamos remover as estruturas aninhadas que estão nas colunas remetente e destinatário*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_flatten = df.withColumns({\n",
    "    'remetente_nome': col('remetente').getField('nome'),\n",
    "    'remetente_banco': col('remetente').getField('banco'),\n",
    "    'remetente_tipo': col('remetente').getField('tipo'),\n",
    "\n",
    "    'destinatario_nome': col('destinatario').getField('nome'),\n",
    "    'destinatario_banco': col('destinatario').getField('banco'),\n",
    "    'destinatario_tipo': col('destinatario').getField('tipo'),\n",
    "    }).drop('remetente', 'destinatario')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_transacao: integer (nullable = true)\n",
      " |-- valor: double (nullable = true)\n",
      " |-- categoria: string (nullable = true)\n",
      " |-- transaction_date: string (nullable = true)\n",
      " |-- chave_pix: string (nullable = true)\n",
      " |-- fraude: integer (nullable = true)\n",
      " |-- remetente_nome: string (nullable = true)\n",
      " |-- remetente_banco: string (nullable = true)\n",
      " |-- remetente_tipo: string (nullable = true)\n",
      " |-- destinatario_nome: string (nullable = true)\n",
      " |-- destinatario_banco: string (nullable = true)\n",
      " |-- destinatario_tipo: string (nullable = true)\n",
      "\n",
      "None\n",
      "+------------+------------------+-------------+-------------------+---------+------+------------------+---------------+--------------+--------------------+------------------+-----------------+\n",
      "|id_transacao|             valor|    categoria|   transaction_date|chave_pix|fraude|    remetente_nome|remetente_banco|remetente_tipo|   destinatario_nome|destinatario_banco|destinatario_tipo|\n",
      "+------------+------------------+-------------+-------------------+---------+------+------------------+---------------+--------------+--------------------+------------------+-----------------+\n",
      "|        1000|            588.08|       outros|2021-07-16 05:00:55|aleatoria|     0|Jonathan Gonsalves|            BTG|            PF|         Calebe Melo|             Caixa|               PF|\n",
      "|        1001|           80682.5|transferencia|2022-04-20 12:34:01|  celular|     1|Jonathan Gonsalves|            BTG|            PF|  Davi Lucas Pereira|             Caixa|               PJ|\n",
      "|        1002|             549.9|        lazer|2022-07-10 16:51:34|      cpf|     0|Jonathan Gonsalves|            BTG|            PF|      Sabrina Castro|            Nubank|               PF|\n",
      "|        1003|             90.83|   transporte|2022-10-20 10:57:36|aleatoria|     0|Jonathan Gonsalves|            BTG|            PF|Francisco da Conc...|            Nubank|               PJ|\n",
      "|        1004|13272.619999999999|transferencia|2021-04-06 20:26:51|    email|     0|Jonathan Gonsalves|            BTG|            PF|   Isabelly Ferreira|               BTG|               PJ|\n",
      "+------------+------------------+-------------+-------------------+---------+------+------------------+---------------+--------------+--------------------+------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df_flatten.printSchema())\n",
    "print(df_flatten.show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sumarização dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/27 13:59:26 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+-----------+-------------------+---------+------------------+------------------+---------------+--------------+-----------------+------------------+-----------------+\n",
      "|summary|     id_transacao|             valor|  categoria|   transaction_date|chave_pix|            fraude|    remetente_nome|remetente_banco|remetente_tipo|destinatario_nome|destinatario_banco|destinatario_tipo|\n",
      "+-------+-----------------+------------------+-----------+-------------------+---------+------------------+------------------+---------------+--------------+-----------------+------------------+-----------------+\n",
      "|  count|           100000|            100000|     100000|             100000|   100000|            100000|            100000|         100000|        100000|           100000|            100000|           100000|\n",
      "|   mean|          50999.5|10303.358732200059|       NULL|               NULL|     NULL|           0.15367|              NULL|           NULL|          NULL|             NULL|              NULL|             NULL|\n",
      "| stddev|28867.65779668774| 20874.99768875586|       NULL|               NULL|     NULL|0.3606339302787737|              NULL|           NULL|          NULL|             NULL|              NULL|             NULL|\n",
      "|    min|             1000|               0.0|alimentacao|2021-01-14 15:37:45|aleatoria|                 0|Jonathan Gonsalves|            BTG|            PF|   Agatha Almeida|               BTG|               PF|\n",
      "|    max|           100999|          89996.33|  vestuario|2023-01-15 02:51:10|    email|                 1|Jonathan Gonsalves|            BTG|            PF|   Yuri das Neves|                XP|               PJ|\n",
      "+-------+-----------------+------------------+-----------+-------------------+---------+------------------+------------------+---------------+--------------+-----------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_flatten.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tnOJ5pwHARTD"
   },
   "source": [
    "## 4 - Modelagem\n",
    "\n",
    "- Para qual banco esse cliente mais transfere?\n",
    "- Qual é a média de transferências por período que esse cliente faz?\n",
    "- Baseando-se no valor das transferências, poderia dar um aumento de crédito?\n",
    "- Para o que esse cliente mais usa as transferências?\n",
    "- Executar um algoritmo de machine learning que identifique possíveis transações com fraude.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|destinatario_banco|count|\n",
      "+------------------+-----+\n",
      "|                XP|14401|\n",
      "|               BTG|14390|\n",
      "|            Nubank|14297|\n",
      "|              Itau|14281|\n",
      "|             Caixa|14240|\n",
      "|                C6|14204|\n",
      "|          Bradesco|14187|\n",
      "+------------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Para qual banco foram feitas mais transações?\n",
    "df_flatten.groupBy('destinatario_banco').count().orderBy(col('count').desc()).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----+\n",
      "|ano_mes|destinatario_banco|count|\n",
      "+-------+------------------+-----+\n",
      "|2023-01|              Itau|  267|\n",
      "|2023-01|             Caixa|  277|\n",
      "|2023-01|                XP|  277|\n",
      "|2023-01|          Bradesco|  280|\n",
      "|2023-01|            Nubank|  290|\n",
      "|2023-01|                C6|  290|\n",
      "|2023-01|               BTG|  278|\n",
      "|2022-12|                XP|  615|\n",
      "|2022-12|               BTG|  603|\n",
      "|2022-12|                C6|  576|\n",
      "|2022-12|          Bradesco|  575|\n",
      "|2022-12|            Nubank|  602|\n",
      "|2022-12|              Itau|  633|\n",
      "|2022-12|             Caixa|  616|\n",
      "|2022-11|          Bradesco|  579|\n",
      "|2022-11|               BTG|  580|\n",
      "|2022-11|              Itau|  614|\n",
      "|2022-11|            Nubank|  620|\n",
      "|2022-11|             Caixa|  543|\n",
      "|2022-11|                C6|  561|\n",
      "+-------+------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# total de transações por mês e por banco\n",
    "\n",
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "df_flatten.groupBy(\n",
    "    date_format('transaction_date', 'yyyy-MM').alias('ano_mes'), 'destinatario_banco'\n",
    "    ).count().orderBy(col('ano_mes').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+\n",
      "|destinatario_banco|avg(valor)|\n",
      "+------------------+----------+\n",
      "|            Nubank| 10,316.48|\n",
      "|                C6| 10,309.50|\n",
      "|               BTG| 10,122.30|\n",
      "|                XP| 10,328.07|\n",
      "|             Caixa| 10,254.86|\n",
      "|          Bradesco| 10,564.19|\n",
      "|              Itau| 10,230.88|\n",
      "+------------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, avg, format_number, count\n",
    "\n",
    "# Valor de transação médio para cada banco\n",
    "#df_flatten.groupBy('destinatario_banco').avg('valor').orderBy(col('avg(valor)').asc()).show()\n",
    "average_df = df_flatten.groupBy('destinatario_banco').avg('valor')\n",
    "formatted_average_df = average_df.withColumn('avg(valor)', format_number(col('avg(valor)'), 2))\n",
    "formatted_average_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+\n",
      "|destinatario_banco|    sum(valor)|\n",
      "+------------------+--------------+\n",
      "|            Nubank|147,494,648.81|\n",
      "|                C6|146,436,134.80|\n",
      "|               BTG|145,659,894.17|\n",
      "|                XP|148,734,558.71|\n",
      "|             Caixa|146,029,263.58|\n",
      "|          Bradesco|149,874,228.63|\n",
      "|              Itau|146,107,144.52|\n",
      "+------------------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Valor de transação total para cada banco\n",
    "sum_df = df_flatten.groupBy('destinatario_banco').sum('valor')\n",
    "formatted_sum_df = sum_df.withColumn('sum(valor)', format_number(col('sum(valor)'), 2))\n",
    "formatted_sum_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------+-----+\n",
      "|ano_mes|destinatario_banco|    categoria|count|\n",
      "+-------+------------------+-------------+-----+\n",
      "|2023-01|            Nubank|     educacao|   21|\n",
      "|2023-01|               BTG|    presentes|   22|\n",
      "|2023-01|              Itau|       outros|   27|\n",
      "|2023-01|                XP|   transporte|   32|\n",
      "|2023-01|               BTG|     educacao|   37|\n",
      "|2023-01|            Nubank|        lazer|   29|\n",
      "|2023-01|             Caixa|     educacao|   26|\n",
      "|2023-01|             Caixa|        lazer|   21|\n",
      "|2023-01|               BTG|        saude|   28|\n",
      "|2023-01|                XP|        lazer|   26|\n",
      "|2023-01|              Itau|     educacao|   30|\n",
      "|2023-01|             Caixa|    presentes|   31|\n",
      "|2023-01|            Nubank|    presentes|   25|\n",
      "|2023-01|             Caixa|  alimentacao|   31|\n",
      "|2023-01|                C6|  alimentacao|   30|\n",
      "|2023-01|            Nubank|        saude|   34|\n",
      "|2023-01|            Nubank|       outros|   35|\n",
      "|2023-01|            Nubank|   transporte|   28|\n",
      "|2023-01|                C6|transferencia|   63|\n",
      "|2023-01|          Bradesco|       outros|   22|\n",
      "+-------+------------------+-------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# total de transações por mês/banco por categoria\n",
    "\n",
    "df_flatten.groupBy(\n",
    "    date_format('transaction_date', 'yyyy-MM').alias('ano_mes'), 'destinatario_banco', 'categoria'\n",
    "    ).count().orderBy(col('ano_mes').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+-----+\n",
      "| ano|    categoria|count|\n",
      "+----+-------------+-----+\n",
      "|2023|        saude|  193|\n",
      "|2023|    vestuario|  174|\n",
      "|2023|     educacao|  202|\n",
      "|2023|    presentes|  163|\n",
      "|2023|   transporte|  178|\n",
      "|2023|  alimentacao|  189|\n",
      "|2023|        lazer|  193|\n",
      "|2023|transferencia|  475|\n",
      "|2023|       outros|  192|\n",
      "|2022|       outros| 4702|\n",
      "|2022|        saude| 4784|\n",
      "|2022|        lazer| 4784|\n",
      "|2022|  alimentacao| 4799|\n",
      "|2022|   transporte| 4593|\n",
      "|2022|transferencia|12269|\n",
      "|2022|    vestuario| 4731|\n",
      "|2022|     educacao| 4681|\n",
      "|2022|    presentes| 4687|\n",
      "|2021|  alimentacao| 4560|\n",
      "|2021|    presentes| 4404|\n",
      "+----+-------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# total de transações por categoria/ano\n",
    "\n",
    "df_flatten.groupBy(\n",
    "    date_format('transaction_date', 'yyyy').alias('ano'), 'categoria'\n",
    "    ).count().orderBy(col('ano').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|    categoria|count|\n",
      "+-------------+-----+\n",
      "|transferencia|24744|\n",
      "|  alimentacao| 9548|\n",
      "|    vestuario| 9503|\n",
      "|        saude| 9476|\n",
      "|        lazer| 9464|\n",
      "|     educacao| 9460|\n",
      "|       outros| 9377|\n",
      "|    presentes| 9254|\n",
      "|   transporte| 9174|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# total de transações por categoria\n",
    "\n",
    "df_flatten.groupBy('categoria'\n",
    "    ).count().orderBy(col('count').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| ano|count|\n",
      "+----+-----+\n",
      "|2023| 1959|\n",
      "|2022|50030|\n",
      "|2021|48011|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Conta o número de transações por ano\n",
    "df_flatten.groupBy(date_format(col(\"transaction_date\"), \"yyyy\").alias(\"ano\")).agg(\n",
    "    count(\"id_transacao\").alias(\"count\")\n",
    ").orderBy(\"ano\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+\n",
      "| ano|         total|\n",
      "+----+--------------+\n",
      "|2023| 19,594,633.67|\n",
      "|2022|513,575,644.77|\n",
      "|2021|497,165,594.78|\n",
      "+----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# valor total de transações por ano\n",
    "df_flatten.groupBy(date_format(col(\"transaction_date\"), \"yyyy\").alias(\"ano\")).sum(\n",
    "    \"valor\"\n",
    ").select(\"ano\", format_number(col(\"sum(valor)\"), 2).alias(\"total\")).orderBy(\n",
    "    \"ano\", ascending=False\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "| ano|      avg|\n",
      "+----+---------+\n",
      "|2023|10,002.37|\n",
      "|2022|10,265.35|\n",
      "|2021|10,355.24|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# valor médio de transações por ano\n",
    "df_flatten.groupBy(date_format(col(\"transaction_date\"), \"yyyy\").alias(\"ano\")).avg(\n",
    "    \"valor\"\n",
    ").select(\"ano\", format_number(col(\"avg(valor)\"), 2).alias(\"avg\")).orderBy(\n",
    "    \"ano\", ascending=False\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|fraude|count|\n",
      "+------+-----+\n",
      "|     1|15367|\n",
      "|     0|84633|\n",
      "+------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Quantidade de fraudes\n",
    "df_flatten.groupBy('fraude').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "| ano|   total|\n",
      "+----+--------+\n",
      "|2023|  284.00|\n",
      "|2022|7,642.00|\n",
      "|2021|7,441.00|\n",
      "+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Quantidade de fraudes por ano\n",
    "df_flatten.filter(col(\"fraude\") == 1).groupBy(\n",
    "    date_format(col(\"transaction_date\"), \"yyyy\").alias(\"ano\")\n",
    ").count().select(\"ano\", format_number(col(\"count\"), 2).alias(\"total\")).orderBy(\n",
    "    \"ano\", ascending=False\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+\n",
      "|  ano|    total|\n",
      "+-----+---------+\n",
      "| 2023|   284.00|\n",
      "| 2022| 7,642.00|\n",
      "| 2021| 7,441.00|\n",
      "|Total|15,367.00|\n",
      "+-----+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#verificar o total de fraudes por ano e tirar a prova dos valores comparados ao método anteriror\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Conta o número de fraudes por ano\n",
    "df_yearly = df_flatten.filter(col(\"fraude\") == 1).groupBy(\n",
    "    date_format(col(\"transaction_date\"), \"yyyy\").alias(\"ano\")\n",
    ").count().select(\n",
    "    \"ano\", format_number(col(\"count\"), 2).alias(\"total\")\n",
    ").orderBy(\n",
    "    \"ano\", ascending=False\n",
    ")\n",
    "\n",
    "# Conta o número total de fraudes\n",
    "df_total = df_flatten.filter(col(\"fraude\") == 1).select(\n",
    "    lit(\"Total\").alias(\"ano\"), format_number(count(\"*\"), 2).alias(\"total\")\n",
    ")\n",
    "\n",
    "# Adiciona a linha total ao DataFrame\n",
    "df_result = df_yearly.union(df_total)\n",
    "\n",
    "# Mostra o resultado\n",
    "df_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+-----+\n",
      "|fraude|    categoria|count|\n",
      "+------+-------------+-----+\n",
      "|     0|    vestuario| 9503|\n",
      "|     0|   transporte| 9174|\n",
      "|     1|transferencia|15367|\n",
      "|     0|transferencia| 9377|\n",
      "|     0|        saude| 9476|\n",
      "|     0|    presentes| 9254|\n",
      "|     0|       outros| 9377|\n",
      "|     0|        lazer| 9464|\n",
      "|     0|     educacao| 9460|\n",
      "|     0|  alimentacao| 9548|\n",
      "+------+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Agrupamento por cateria e fraude\n",
    "df_flatten.groupBy('fraude', 'categoria').count().orderBy('categoria', ascending =False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|range|count|\n",
      "+-----+-----+\n",
      "|19000|    1|\n",
      "|20000|  242|\n",
      "|21000|  231|\n",
      "|22000|  227|\n",
      "|23000|  230|\n",
      "|24000|  195|\n",
      "|25000|  233|\n",
      "|26000|  227|\n",
      "|27000|  242|\n",
      "|28000|  222|\n",
      "|29000|  233|\n",
      "|30000|  207|\n",
      "|31000|  242|\n",
      "|32000|  192|\n",
      "|33000|  207|\n",
      "|34000|  203|\n",
      "|35000|  254|\n",
      "|36000|  253|\n",
      "|37000|  252|\n",
      "|38000|  221|\n",
      "+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# faixa de valores em que ocorreram fraudes\n",
    "\n",
    "from pyspark.sql.functions import floor\n",
    "\n",
    "df_flatten.filter(col(\"fraude\") == 1).withColumn(\n",
    "    \"range\", floor(col(\"valor\") / 1000) * 1000\n",
    ").groupBy(\"range\").count().orderBy(\"range\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+\n",
      "|faixa_max_fraude|faixa_min_fraude|\n",
      "+----------------+----------------+\n",
      "|           89000|           19000|\n",
      "+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Faixa máxima e mínima de valores que ocorreram fraudes\n",
    "\n",
    "from pyspark.sql.functions import floor, max, min\n",
    "\n",
    "df_flatten.filter(col(\"fraude\") == 1).withColumn(\n",
    "    \"range\", floor(col(\"valor\") / 1000) * 1000\n",
    ").select(max(\"range\").alias('faixa_max_fraude'), min('range').alias('faixa_min_fraude')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rIVX81NQzsoZ"
   },
   "source": [
    "## 5 - Modelo de Predição de Fraudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install distutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf, round, lit\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id_transacao',\n",
       " 'valor',\n",
       " 'categoria',\n",
       " 'transaction_date',\n",
       " 'chave_pix',\n",
       " 'fraude',\n",
       " 'remetente_nome',\n",
       " 'remetente_banco',\n",
       " 'remetente_tipo',\n",
       " 'destinatario_nome',\n",
       " 'destinatario_banco',\n",
       " 'destinatario_tipo']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_flatten.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id_transacao',\n",
       " 'valor',\n",
       " 'remetente',\n",
       " 'destinatario',\n",
       " 'categoria',\n",
       " 'transaction_date',\n",
       " 'chave_pix',\n",
       " 'fraude']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(\n",
    "    inputCols=[\n",
    "        \"destinatario_nome\", \n",
    "        \"destinatario_banco\",\n",
    "        \"destinatario_tipo\",\n",
    "        \"categoria\",\n",
    "        \"chave_pix\"\n",
    "    ], \n",
    "    outputCols=[\n",
    "        \"destinatario_nome_index\", \n",
    "        \"destinatario_banco_index\",\n",
    "        \"destinatario_tipo_index\",\n",
    "        \"categoria_index\",\n",
    "        \"chave_pix_index\"\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/27 14:01:13 WARN DAGScheduler: Broadcasting large task binary with size 1278.8 KiB\n",
      "[Stage 47:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+-------------+-------------------+---------+------+------------------+---------------+--------------+--------------------+------------------+-----------------+-----------------------+------------------------+-----------------------+---------------+---------------+\n",
      "|id_transacao|             valor|    categoria|   transaction_date|chave_pix|fraude|    remetente_nome|remetente_banco|remetente_tipo|   destinatario_nome|destinatario_banco|destinatario_tipo|destinatario_nome_index|destinatario_banco_index|destinatario_tipo_index|categoria_index|chave_pix_index|\n",
      "+------------+------------------+-------------+-------------------+---------+------+------------------+---------------+--------------+--------------------+------------------+-----------------+-----------------------+------------------------+-----------------------+---------------+---------------+\n",
      "|        1000|            588.08|       outros|2021-07-16 05:00:55|aleatoria|     0|Jonathan Gonsalves|            BTG|            PF|         Calebe Melo|             Caixa|               PF|                12045.0|                     4.0|                    1.0|            6.0|            3.0|\n",
      "|        1001|           80682.5|transferencia|2022-04-20 12:34:01|  celular|     1|Jonathan Gonsalves|            BTG|            PF|  Davi Lucas Pereira|             Caixa|               PJ|                  259.0|                     4.0|                    0.0|            0.0|            2.0|\n",
      "|        1002|             549.9|        lazer|2022-07-10 16:51:34|      cpf|     0|Jonathan Gonsalves|            BTG|            PF|      Sabrina Castro|            Nubank|               PF|                  132.0|                     2.0|                    1.0|            4.0|            1.0|\n",
      "|        1003|             90.83|   transporte|2022-10-20 10:57:36|aleatoria|     0|Jonathan Gonsalves|            BTG|            PF|Francisco da Conc...|            Nubank|               PJ|                10475.0|                     2.0|                    0.0|            8.0|            3.0|\n",
      "|        1004|13272.619999999999|transferencia|2021-04-06 20:26:51|    email|     0|Jonathan Gonsalves|            BTG|            PF|   Isabelly Ferreira|               BTG|               PJ|                 4159.0|                     1.0|                    0.0|            0.0|            0.0|\n",
      "|        1005|           9347.58|        saude|2022-07-24 15:22:27|aleatoria|     0|Jonathan Gonsalves|            BTG|            PF|Srta. Maria da Cunha|              Itau|               PJ|                26853.0|                     3.0|                    0.0|            3.0|            3.0|\n",
      "|        1006|           7836.76|    presentes|2022-10-05 19:20:24|      cpf|     0|Jonathan Gonsalves|            BTG|            PF|     Catarina Duarte|                C6|               PF|                 5578.0|                     5.0|                    1.0|            7.0|            1.0|\n",
      "|        1007|           3883.62|    vestuario|2021-04-24 17:36:34|      cpf|     0|Jonathan Gonsalves|            BTG|            PF|       Vitor Correia|                XP|               PJ|                13528.0|                     0.0|                    0.0|            2.0|            1.0|\n",
      "|        1008|               4.0|        saude|2021-11-16 21:46:47|aleatoria|     0|Jonathan Gonsalves|            BTG|            PF|         Theo Novaes|                C6|               PJ|                 1141.0|                     5.0|                    0.0|            3.0|            3.0|\n",
      "|        1009|              24.3|transferencia|2021-07-26 02:08:49|      cpf|     0|Jonathan Gonsalves|            BTG|            PF|     Isabel Caldeira|                XP|               PJ|                 8369.0|                     0.0|                    0.0|            0.0|            1.0|\n",
      "|        1010|           87555.3|transferencia|2022-03-14 15:34:45|aleatoria|     1|Jonathan Gonsalves|            BTG|            PF|Sr. Henrique Cardoso|            Nubank|               PF|                22115.0|                     2.0|                    1.0|            0.0|            3.0|\n",
      "|        1011|          21345.91|transferencia|2021-10-31 04:31:51|      cpf|     1|Jonathan Gonsalves|            BTG|            PF|   Felipe Cavalcanti|            Nubank|               PJ|                 5897.0|                     2.0|                    0.0|            0.0|            1.0|\n",
      "|        1012|          73605.85|transferencia|2021-04-30 19:19:56|  celular|     1|Jonathan Gonsalves|            BTG|            PF|     Dr. Davi da Luz|          Bradesco|               PJ|                12212.0|                     6.0|                    0.0|            0.0|            2.0|\n",
      "|        1013|             93.53|  alimentacao|2023-01-13 13:39:57|      cpf|     0|Jonathan Gonsalves|            BTG|            PF|    Stephany Cardoso|                C6|               PJ|                   10.0|                     5.0|                    0.0|            1.0|            1.0|\n",
      "|        1014|            564.11|    vestuario|2022-05-27 23:06:08|aleatoria|     0|Jonathan Gonsalves|            BTG|            PF|   Sra. Julia Araujo|              Itau|               PJ|                24413.0|                     3.0|                    0.0|            2.0|            3.0|\n",
      "|        1015|              3.59|        saude|2021-10-06 21:19:58|    email|     0|Jonathan Gonsalves|            BTG|            PF|     Carolina Farias|            Nubank|               PJ|                10121.0|                     2.0|                    0.0|            3.0|            0.0|\n",
      "|        1016|          19164.89|  alimentacao|2022-03-06 17:59:43|    email|     0|Jonathan Gonsalves|            BTG|            PF|   Isabelly da Costa|            Nubank|               PJ|                 6122.0|                     2.0|                    0.0|            1.0|            0.0|\n",
      "|        1017|             68.45|    vestuario|2022-04-01 18:17:40|aleatoria|     0|Jonathan Gonsalves|            BTG|            PF|Joao Miguel Silveira|                C6|               PJ|                 6214.0|                     5.0|                    0.0|            2.0|            3.0|\n",
      "|        1018|            941.25|    vestuario|2022-05-23 00:28:13|  celular|     0|Jonathan Gonsalves|            BTG|            PF|       Matheus Moura|                C6|               PF|                 1829.0|                     5.0|                    1.0|            2.0|            2.0|\n",
      "|        1019|27009.910000000003|transferencia|2021-08-04 23:22:37|    email|     1|Jonathan Gonsalves|            BTG|            PF| Gabrielly Goncalves|              Itau|               PJ|                 5973.0|                     3.0|                    0.0|            0.0|            0.0|\n",
      "+------------+------------------+-------------+-------------------+---------+------+------------------+---------------+--------------+--------------------+------------------+-----------------+-----------------------+------------------------+-----------------------+---------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_index = indexer.fit(df_flatten).transform(df_flatten)\n",
    "df_index.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para filtros, podemos usar somente colunas numéricas e de data\n",
    "cols_para_filtrar = [\n",
    "  \"valor\",\n",
    "  \"transaction_date\",\n",
    "  \"destinatario_nome_index\", \n",
    "  \"destinatario_banco_index\",\n",
    "  \"destinatario_tipo_index\",\n",
    "  \"chave_pix_index\",\n",
    "  \"categoria_index\",\n",
    "  \"fraude\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_fraud = df_index.select(cols_para_filtrar).filter(col(\"fraude\") == 1)\n",
    "not_fraud = df_index.select(cols_para_filtrar).filter(col(\"fraude\") == 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separar amostra dos dados de fraude\n",
    "not_fraud = not_fraud.sample(False, 0.1, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23712"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat = not_fraud.union(is_fraud)\n",
    "df = df_concat.sort(\"transaction_date\")\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/27 14:01:40 WARN DAGScheduler: Broadcasting large task binary with size 1290.0 KiB\n",
      "24/05/27 14:01:42 WARN DAGScheduler: Broadcasting large task binary with size 1302.2 KiB\n",
      "24/05/27 14:01:44 WARN DAGScheduler: Broadcasting large task binary with size 1296.6 KiB\n",
      "24/05/27 14:01:44 WARN DAGScheduler: Broadcasting large task binary with size 1290.0 KiB\n",
      "24/05/27 14:01:46 WARN DAGScheduler: Broadcasting large task binary with size 1302.2 KiB\n",
      "24/05/27 14:01:48 WARN DAGScheduler: Broadcasting large task binary with size 1296.6 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train = 16504  test = 7208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train, test = df.randomSplit([0.7, 0.3], seed = 123)\n",
    "print(\"train =\", train.count(), \" test =\", test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_fraud = udf(lambda fraud: 1.0 if fraud > 0 else 0.0, DoubleType())\n",
    "train = train.withColumn(\"is_fraud\", is_fraud(train.fraude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = train.repartition(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.conf.set(\"spark.executor.heartbeatInterval\", \"60s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/27 14:02:05 WARN DAGScheduler: Broadcasting large task binary with size 1290.0 KiB\n",
      "24/05/27 14:02:07 WARN DAGScheduler: Broadcasting large task binary with size 1302.2 KiB\n",
      "[Stage 66:==============>                                           (1 + 2) / 4]\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/27 14:02:25 WARN DAGScheduler: Broadcasting large task binary with size 1290.0 KiB\n",
      "24/05/27 14:02:26 WARN DAGScheduler: Broadcasting large task binary with size 1302.2 KiB\n",
      "24/05/27 14:02:28 WARN DAGScheduler: Broadcasting large task binary with size 2041.4 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train.write.mode(\"overwrite\").parquet(f\"data{os.sep}train.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/27 14:02:58 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "24/05/27 14:02:58 WARN DAGScheduler: Broadcasting large task binary with size 1290.0 KiB\n",
      "24/05/27 14:03:00 WARN DAGScheduler: Broadcasting large task binary with size 1302.2 KiB\n",
      "24/05/27 14:03:02 WARN DAGScheduler: Broadcasting large task binary with size 1290.0 KiB\n",
      "24/05/27 14:03:03 WARN DAGScheduler: Broadcasting large task binary with size 1302.2 KiB\n",
      "24/05/27 14:03:05 WARN DAGScheduler: Broadcasting large task binary with size 1340.8 KiB\n",
      "24/05/27 14:03:07 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/05/27 14:03:07 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "24/05/27 14:03:07 WARN DAGScheduler: Broadcasting large task binary with size 1341.5 KiB\n",
      "24/05/27 14:03:07 WARN DAGScheduler: Broadcasting large task binary with size 1341.5 KiB\n",
      "24/05/27 14:03:08 WARN DAGScheduler: Broadcasting large task binary with size 1341.5 KiB\n",
      "24/05/27 14:03:08 WARN DAGScheduler: Broadcasting large task binary with size 1341.5 KiB\n",
      "24/05/27 14:03:08 WARN DAGScheduler: Broadcasting large task binary with size 1341.5 KiB\n",
      "24/05/27 14:03:08 WARN DAGScheduler: Broadcasting large task binary with size 1341.5 KiB\n",
      "24/05/27 14:03:08 WARN DAGScheduler: Broadcasting large task binary with size 1341.5 KiB\n",
      "24/05/27 14:03:08 WARN DAGScheduler: Broadcasting large task binary with size 1341.5 KiB\n",
      "24/05/27 14:03:08 WARN DAGScheduler: Broadcasting large task binary with size 1341.5 KiB\n",
      "24/05/27 14:03:08 WARN DAGScheduler: Broadcasting large task binary with size 1341.5 KiB\n",
      "24/05/27 14:03:08 WARN DAGScheduler: Broadcasting large task binary with size 1341.5 KiB\n",
      "24/05/27 14:03:08 WARN DAGScheduler: Broadcasting large task binary with size 1341.5 KiB\n",
      "24/05/27 14:03:08 WARN DAGScheduler: Broadcasting large task binary with size 1341.5 KiB\n",
      "24/05/27 14:03:08 WARN DAGScheduler: Broadcasting large task binary with size 1341.5 KiB\n",
      "24/05/27 14:03:08 WARN DAGScheduler: Broadcasting large task binary with size 1341.5 KiB\n",
      "24/05/27 14:03:09 WARN DAGScheduler: Broadcasting large task binary with size 1341.5 KiB\n",
      "24/05/27 14:03:09 WARN DAGScheduler: Broadcasting large task binary with size 1341.5 KiB\n",
      "24/05/27 14:03:09 WARN DAGScheduler: Broadcasting large task binary with size 1341.5 KiB\n",
      "24/05/27 14:03:09 WARN DAGScheduler: Broadcasting large task binary with size 1341.5 KiB\n",
      "24/05/27 14:03:09 WARN DAGScheduler: Broadcasting large task binary with size 1341.5 KiB\n",
      "24/05/27 14:03:09 WARN DAGScheduler: Broadcasting large task binary with size 1341.5 KiB\n",
      "24/05/27 14:03:09 WARN DAGScheduler: Broadcasting large task binary with size 1341.5 KiB\n",
      "24/05/27 14:03:09 WARN DAGScheduler: Broadcasting large task binary with size 1341.5 KiB\n",
      "24/05/27 14:03:09 WARN DAGScheduler: Broadcasting large task binary with size 1341.5 KiB\n",
      "24/05/27 14:03:09 WARN DAGScheduler: Broadcasting large task binary with size 1341.5 KiB\n",
      "24/05/27 14:03:10 WARN DAGScheduler: Broadcasting large task binary with size 1341.5 KiB\n",
      "24/05/27 14:03:10 WARN DAGScheduler: Broadcasting large task binary with size 1341.5 KiB\n",
      "24/05/27 14:03:10 WARN DAGScheduler: Broadcasting large task binary with size 1341.5 KiB\n",
      "24/05/27 14:03:10 WARN DAGScheduler: Broadcasting large task binary with size 1341.5 KiB\n",
      "24/05/27 14:03:10 WARN DAGScheduler: Broadcasting large task binary with size 1341.5 KiB\n",
      "24/05/27 14:03:10 WARN DAGScheduler: Broadcasting large task binary with size 1341.5 KiB\n",
      "24/05/27 14:03:10 WARN DAGScheduler: Broadcasting large task binary with size 1341.5 KiB\n",
      "24/05/27 14:03:10 WARN DAGScheduler: Broadcasting large task binary with size 1341.5 KiB\n",
      "24/05/27 14:03:11 WARN DAGScheduler: Broadcasting large task binary with size 1341.5 KiB\n",
      "24/05/27 14:03:11 WARN DAGScheduler: Broadcasting large task binary with size 1341.5 KiB\n",
      "24/05/27 14:03:11 WARN DAGScheduler: Broadcasting large task binary with size 1341.5 KiB\n",
      "24/05/27 14:03:11 WARN DAGScheduler: Broadcasting large task binary with size 1341.5 KiB\n",
      "24/05/27 14:03:11 WARN DAGScheduler: Broadcasting large task binary with size 1341.5 KiB\n",
      "24/05/27 14:03:11 WARN DAGScheduler: Broadcasting large task binary with size 1341.5 KiB\n",
      "24/05/27 14:03:11 WARN DAGScheduler: Broadcasting large task binary with size 1341.5 KiB\n",
      "24/05/27 14:03:11 WARN DAGScheduler: Broadcasting large task binary with size 1341.5 KiB\n",
      "24/05/27 14:03:11 WARN DAGScheduler: Broadcasting large task binary with size 1341.5 KiB\n",
      "24/05/27 14:03:11 WARN DAGScheduler: Broadcasting large task binary with size 1341.5 KiB\n",
      "24/05/27 14:03:11 WARN DAGScheduler: Broadcasting large task binary with size 1341.5 KiB\n",
      "24/05/27 14:03:11 WARN DAGScheduler: Broadcasting large task binary with size 1341.5 KiB\n",
      "24/05/27 14:03:11 WARN DAGScheduler: Broadcasting large task binary with size 1341.5 KiB\n",
      "24/05/27 14:03:11 WARN DAGScheduler: Broadcasting large task binary with size 1341.5 KiB\n",
      "24/05/27 14:03:12 WARN DAGScheduler: Broadcasting large task binary with size 1290.0 KiB\n",
      "24/05/27 14:03:13 WARN DAGScheduler: Broadcasting large task binary with size 1302.2 KiB\n",
      "24/05/27 14:03:15 WARN DAGScheduler: Broadcasting large task binary with size 1290.0 KiB\n",
      "24/05/27 14:03:16 WARN DAGScheduler: Broadcasting large task binary with size 1302.2 KiB\n",
      "[Stage 174:============================>                            (2 + 2) / 4]\r"
     ]
    }
   ],
   "source": [
    "# Create the feature vectors.\n",
    "assembler = VectorAssembler(\n",
    "  inputCols = [x for x in train.columns if x not in [\"transaction_date\", \"fraude\", \"is_fraud\"]],\n",
    "  outputCol = \"features\")\n",
    "\n",
    "# Use Logistic Regression.\n",
    "lr = LogisticRegression().setParams(\n",
    "    maxIter = 100000,\n",
    "    labelCol = \"is_fraud\",\n",
    "    predictionCol = \"prediction\")\n",
    "\n",
    "spark = spark.builder.config(\"spark.network.timeout\", \"600s\").getOrCreate()\n",
    "\n",
    "# Repartition the train DataFrame into 4 partitions\n",
    "# train = train.repartition()\n",
    "\n",
    "# This will train a logistic regression model on the input data and return a \n",
    "# LogisticRegressionModel object which can be used to make predictions on new data.\n",
    "model = Pipeline(stages = [assembler, lr]).fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/27 14:25:16 WARN DAGScheduler: Broadcasting large task binary with size 1290.0 KiB\n",
      "24/05/27 14:25:17 WARN DAGScheduler: Broadcasting large task binary with size 1302.2 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/27 14:25:18 WARN DAGScheduler: Broadcasting large task binary with size 1339.8 KiB\n",
      "24/05/27 14:25:19 WARN DAGScheduler: Broadcasting large task binary with size 1330.9 KiB\n",
      "24/05/27 14:25:19 WARN DAGScheduler: Broadcasting large task binary with size 1330.6 KiB\n",
      "24/05/27 14:25:19 WARN DAGScheduler: Broadcasting large task binary with size 1328.2 KiB\n",
      "24/05/27 14:25:19 WARN DAGScheduler: Broadcasting large task binary with size 1290.0 KiB\n",
      "24/05/27 14:25:21 WARN DAGScheduler: Broadcasting large task binary with size 1302.2 KiB\n",
      "24/05/27 14:25:22 WARN DAGScheduler: Broadcasting large task binary with size 1346.0 KiB\n",
      "24/05/27 14:25:22 WARN DAGScheduler: Broadcasting large task binary with size 1334.1 KiB\n",
      "24/05/27 14:25:22 WARN DAGScheduler: Broadcasting large task binary with size 1344.4 KiB\n",
      "24/05/27 14:25:23 WARN DAGScheduler: Broadcasting large task binary with size 1290.0 KiB\n",
      "24/05/27 14:25:24 WARN DAGScheduler: Broadcasting large task binary with size 1302.2 KiB\n",
      "24/05/27 14:25:25 WARN DAGScheduler: Broadcasting large task binary with size 1346.7 KiB\n",
      "24/05/27 14:25:26 WARN DAGScheduler: Broadcasting large task binary with size 1334.7 KiB\n",
      "24/05/27 14:25:26 WARN DAGScheduler: Broadcasting large task binary with size 1340.5 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+----+-----------+-----------+\n",
      "|is_fraud_prediction| 0_0| 1_0|0_0_percent|1_0_percent|\n",
      "+-------------------+----+----+-----------+-----------+\n",
      "|                1.0|   0|4636|        0.0|      64.31|\n",
      "|                0.0|2571|   1|      35.66|       0.01|\n",
      "+-------------------+----+----+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transforma os dados de teste usando o modelo\n",
    "predicted = model.transform(test)\n",
    "\n",
    "# Adiciona uma nova coluna 'is_fraud' ao DataFrame\n",
    "predicted = predicted.withColumn(\"is_fraud\", is_fraud(predicted.fraude))\n",
    "\n",
    "# Cria uma tabela de contingência (crosstab) entre 'is_fraud' e 'prediction'\n",
    "crosstab_df = predicted.crosstab(\"is_fraud\", \"prediction\")\n",
    "\n",
    "# Converte os nomes das colunas para strings\n",
    "crosstab_df = crosstab_df.toDF(*(c.replace('.', '_') for c in crosstab_df.columns))\n",
    "\n",
    "# Calcula a soma total de todas as linhas\n",
    "total = crosstab_df.select(*(col(c).cast(\"int\") for c in crosstab_df.columns)).rdd.flatMap(lambda x: x).reduce(lambda x, y: x + y)\n",
    "\n",
    "# Adiciona novas colunas ao DataFrame para mostrar as porcentagens\n",
    "for column in crosstab_df.columns[1:]:\n",
    "    crosstab_df = crosstab_df.withColumn(column + '_percent', round((col(column) / lit(total)) * 100, 2))\n",
    "\n",
    "crosstab_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este modelo acertou quase que 100% os casos de fraude, exceto em 1 caso, marcando como falso negativo (onde não era fraude, mas ele determinou que era).\n",
    "\n",
    "Para este caso em específico, deveremos analisar o algoritmo para verificar o motivo desta predição errada. Mas para este projeto, foi considerado aceitável esta pequena margem de erro, em comparação aos erros que ocorriam com os dados reais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2zHoQJWtATfX"
   },
   "source": [
    "# Avaliação do Modelo\n",
    "Será que seu modelo atinge todas as necessidades que foram definidas inicialmente? (e.g. pessoa em cima da bicicleta muda o resultado final)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOTAB1JSAVPi"
   },
   "source": [
    "# Deployment\n",
    "Apresente o relatório com os resultados obtidos.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
