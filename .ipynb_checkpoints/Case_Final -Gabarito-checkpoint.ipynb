{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUrfUdhmyVpV"
      },
      "source": [
        "# Case Final\n",
        "## Análise de Transações PIX\n",
        "CRISP-DM - https://www.escoladnc.com.br/blog/data-science/metodologia-crisp-dm/\n",
        "### Objetivos\n",
        "- Limpar e pré-processar os dados das transações PIX\n",
        "- Analisar padrões de uso do PIX, tais como os canais mais utilizados e os valores de transação mais comuns\n",
        "- Use o PySpark MLlib para treinar e avaliar um modelo de detecção de fraude\n",
        "- Avaliar o desempenho do modelo e fazer recomendações para melhorias futuras\n",
        "\n",
        "### Dados\n",
        "\n",
        "O conjunto de dados inclui as seguintes informações para cada transação:\n",
        "- Detalhes da transação: valor, tempo, remetente e receptor CPF/CNPJ, tipo\n",
        "- Etiqueta de fraude: uma variável binária que indica se a transação foi fraudulenta (1) ou não (0)\n",
        "\n",
        "### Tarefas\n",
        "- Normalização dos dados:\n",
        "  - O dataset que você lerá está em formato json.\n",
        "  ```json\n",
        "  {\n",
        "      \"id_transacao\": inteiro,\n",
        "      \"valor\": texto,\n",
        "      \"remetente\": {\n",
        "          \"nome\": texto,\n",
        "          \"banco\": texto,\n",
        "          \"tipo\": texto\n",
        "      }, \n",
        "      \"destinatario\": {\n",
        "          \"nome\": texto, \n",
        "          \"banco\":texto,\n",
        "          \"tipo\": texto\n",
        "      },          \n",
        "      \"categoria\":texto,\n",
        "      \"chave_pix\":texto,\n",
        "      \"transaction_date\":texto,\n",
        "      \"fraude\":inteiro,\n",
        "  }\n",
        "    ```\n",
        "  - Faça sua transformação para formato colunar\n",
        "- Análise Exploratória de Dados: Use o PySpark para analisar padrões de uso do PIX: \n",
        "  - chaves pix mais usadas;\n",
        "  - os valores de transação mais comuns;\n",
        "  - distribuição dos valores de transação por hora e dia;\n",
        "  - quais bancos receberam mais transferências por dia;\n",
        "  - para qual tipo de pessoa (PF ou PJ) foram realizadas mais transações\n",
        "- Engenharia de Recursos: Apresentar novas características que podem ser úteis para a detecção de fraudes, tais como o número de transações feitas pelo mesmo remetente em um período de tempo específico.\n",
        "- Modelagem: Use o PySpark MLlib para treinar e detectar possíveis transações que contenham fraude.\n",
        "\n",
        "### Observação\n",
        "É importante notar que este é um caso simplificado, e em cenários do mundo real você teria que lidar com dados mais complexos, usar técnicas mais avançadas como métodos de conjuntos e considerar o conhecimento de domínio, bem como leis e regulamentos das instituições financeiras no Brasil.\n",
        "\n",
        "\n",
        "### Observação II\n",
        "Não existe resposta 100% correta. É necessário que você use seu pensamento crítico para definir as melhores métricas e análises para o caso. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qEUukChAJ1t"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpXPtmLKKxnk"
      },
      "source": [
        "# Preparação do Ambiente de Desenvolvimento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_pe7tviACFS",
        "outputId": "f5e7ffbd-14ab-4a2f-f9f7-d240bc926d74"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Invalid requirement: '#==3.3.1'\n",
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "'unzip' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "'.' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "'sleep' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "'grep' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "# Instalar a última versão do PySpark\n",
        "!pip install pyspark #==3.3.1\n",
        "\n",
        "# Instalar o NGROK\n",
        "!wget -qnc https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip -n -q ngrok-stable-linux-amd64.zip\n",
        "\n",
        "# Autenticar a sessão do SparkUI com NGROK\n",
        "!./ngrok authtoken 2KBeQEmmd1YNlQ86GGKf3KFOkb3_6sQH7JEnvEhDxwn9A7WnT\n",
        "get_ipython().system_raw('./ngrok http 4050 &')\n",
        "!sleep 10\n",
        "!curl -s http://localhost:4040/api/tunnels | grep -Po 'public_url\":\"(?=https)\\K[^\"]*'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TwAd_J6YuaF",
        "outputId": "5db8a5e0-805b-4ca0-9b68-baca5bd9095e"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFBKW-UUAMOo"
      },
      "source": [
        "# Data Undesrtanting\n",
        "\n",
        "Primeiramente, devemos entender tudo sobre a fonte dos dados\n",
        "- Como o dado chega até nós?\n",
        "- Qual formato virá? \n",
        "- Aonde o processamento será executado (AWS EMR, Cluster On-Premise)? \n",
        "- De quanto em quanto tempo eu preciso gerar esse relatório (mensal, diário, near-real time)?\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"id_transacao\": inteiro,\n",
        "  \"valor\": texto,\n",
        "  \"remetente\": {\n",
        "      \"nome\": texto,\n",
        "      \"banco\": texto,\n",
        "      \"tipo\": texto\n",
        "  }, \n",
        "  \"destinatario\": {\n",
        "      \"nome\": texto, \n",
        "      \"banco\":texto,\n",
        "      \"tipo\": texto\n",
        "  },        \n",
        "  \"categoria\": texto,\n",
        "  \"transaction_date\":texto,\n",
        "  \"chave_pix\":texto,\n",
        "  \"fraude\":inteiro,\n",
        "}\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPvcwRICAPod"
      },
      "source": [
        "# Preparação dos Dados\n",
        "Agora é hora de começar a preparar os dados de acordo com as suas necessidades."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "m2WQTJNDXBLb"
      },
      "outputs": [],
      "source": [
        "# Iniciar a sessão spark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (\n",
        "    SparkSession.builder                  \n",
        "      .config('spark.ui.port', '4050')\n",
        "      .appName(\"CaseFinal\")\n",
        "      .getOrCreate()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "X2riXiraiFw3"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "caminho_json = r'data\\case_final.json'\n",
        "\n",
        "schema_remetente_destinatario = StructType([\n",
        "    StructField('nome', StringType()),\n",
        "    StructField('banco', StringType()),\n",
        "    StructField('tipo', StringType())\n",
        "])\n",
        "\n",
        "\n",
        "schema_base_pix = StructType([\n",
        "    StructField('id_transacao', IntegerType()),\n",
        "    StructField('valor', DoubleType()),\n",
        "    StructField('remetente', schema_remetente_destinatario),\n",
        "    StructField('destinatario', schema_remetente_destinatario),\n",
        "    StructField('chave_pix', StringType()),\n",
        "    StructField('categoria', StringType()),\n",
        "    StructField('transaction_date', StringType()),\n",
        "    StructField('fraude', IntegerType())\n",
        "])\n",
        "\n",
        "\n",
        "# 2022-10-20 10:57:36\n",
        "\n",
        "df = spark.read.json(\n",
        "    caminho_json,\n",
        "    schema=schema_base_pix,\n",
        "    timestampFormat=\"yyyy-MM-dd HH:mm:ss\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VO7gwLpKiw69",
        "outputId": "e398685a-32d2-4da6-a1c9-c692ffcaa65b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- id_transacao: integer (nullable = true)\n",
            " |-- valor: double (nullable = true)\n",
            " |-- remetente: struct (nullable = true)\n",
            " |    |-- nome: string (nullable = true)\n",
            " |    |-- banco: string (nullable = true)\n",
            " |    |-- tipo: string (nullable = true)\n",
            " |-- destinatario: struct (nullable = true)\n",
            " |    |-- nome: string (nullable = true)\n",
            " |    |-- banco: string (nullable = true)\n",
            " |    |-- tipo: string (nullable = true)\n",
            " |-- chave_pix: string (nullable = true)\n",
            " |-- categoria: string (nullable = true)\n",
            " |-- transaction_date: string (nullable = true)\n",
            " |-- fraude: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wvIETgwi3Uf",
        "outputId": "90da316f-1e95-48cd-f9b4-f12a329f7f7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+------------------+--------------------+--------------------+---------+-------------+-------------------+------+\n",
            "|id_transacao|             valor|           remetente|        destinatario|chave_pix|    categoria|   transaction_date|fraude|\n",
            "+------------+------------------+--------------------+--------------------+---------+-------------+-------------------+------+\n",
            "|        1000|            588.08|{Jonathan Gonsalv...|{Calebe Melo, Cai...|aleatoria|       outros|2021-07-16 05:00:55|     0|\n",
            "|        1001|           80682.5|{Jonathan Gonsalv...|{Davi Lucas Perei...|  celular|transferencia|2022-04-20 12:34:01|     1|\n",
            "|        1002|             549.9|{Jonathan Gonsalv...|{Sabrina Castro, ...|      cpf|        lazer|2022-07-10 16:51:34|     0|\n",
            "|        1003|             90.83|{Jonathan Gonsalv...|{Francisco da Con...|aleatoria|   transporte|2022-10-20 10:57:36|     0|\n",
            "|        1004|13272.619999999999|{Jonathan Gonsalv...|{Isabelly Ferreir...|    email|transferencia|2021-04-06 20:26:51|     0|\n",
            "|        1005|           9347.58|{Jonathan Gonsalv...|{Srta. Maria da C...|aleatoria|        saude|2022-07-24 15:22:27|     0|\n",
            "|        1006|           7836.76|{Jonathan Gonsalv...|{Catarina Duarte,...|      cpf|    presentes|2022-10-05 19:20:24|     0|\n",
            "|        1007|           3883.62|{Jonathan Gonsalv...|{Vitor Correia, X...|      cpf|    vestuario|2021-04-24 17:36:34|     0|\n",
            "|        1008|               4.0|{Jonathan Gonsalv...|{Theo Novaes, C6,...|aleatoria|        saude|2021-11-16 21:46:47|     0|\n",
            "|        1009|              24.3|{Jonathan Gonsalv...|{Isabel Caldeira,...|      cpf|transferencia|2021-07-26 02:08:49|     0|\n",
            "|        1010|           87555.3|{Jonathan Gonsalv...|{Sr. Henrique Car...|aleatoria|transferencia|2022-03-14 15:34:45|     1|\n",
            "|        1011|          21345.91|{Jonathan Gonsalv...|{Felipe Cavalcant...|      cpf|transferencia|2021-10-31 04:31:51|     1|\n",
            "|        1012|          73605.85|{Jonathan Gonsalv...|{Dr. Davi da Luz,...|  celular|transferencia|2021-04-30 19:19:56|     1|\n",
            "|        1013|             93.53|{Jonathan Gonsalv...|{Stephany Cardoso...|      cpf|  alimentacao|2023-01-13 13:39:57|     0|\n",
            "|        1014|            564.11|{Jonathan Gonsalv...|{Sra. Julia Arauj...|aleatoria|    vestuario|2022-05-27 23:06:08|     0|\n",
            "|        1015|              3.59|{Jonathan Gonsalv...|{Carolina Farias,...|    email|        saude|2021-10-06 21:19:58|     0|\n",
            "|        1016|          19164.89|{Jonathan Gonsalv...|{Isabelly da Cost...|    email|  alimentacao|2022-03-06 17:59:43|     0|\n",
            "|        1017|             68.45|{Jonathan Gonsalv...|{Joao Miguel Silv...|aleatoria|    vestuario|2022-04-01 18:17:40|     0|\n",
            "|        1018|            941.25|{Jonathan Gonsalv...|{Matheus Moura, C...|  celular|    vestuario|2022-05-23 00:28:13|     0|\n",
            "|        1019|27009.910000000003|{Jonathan Gonsalv...|{Gabrielly Goncal...|    email|transferencia|2021-08-04 23:22:37|     1|\n",
            "+------------+------------------+--------------------+--------------------+---------+-------------+-------------------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "zWkskpDKi6dN"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "df_flatten = df.withColumns({\n",
        "    'remetente_nome':  col('remetente').getField('nome'),\n",
        "    'remetente_banco':  col('remetente').getField('banco'),\n",
        "    'remetente_tipo':  col('remetente').getField('tipo'),\n",
        "    'destinatario_nome':  col('destinatario').getField('nome'),\n",
        "    'destinatario_banco':  col('destinatario').getField('banco'),\n",
        "    'destinatario_tipo':  col('destinatario').getField('tipo'),\n",
        "}).drop('remetente', 'destinatario')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-bLksT9jAE9",
        "outputId": "a26e3e71-42d1-4503-c08a-f0826726a39c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- id_transacao: integer (nullable = true)\n",
            " |-- valor: double (nullable = true)\n",
            " |-- chave_pix: string (nullable = true)\n",
            " |-- categoria: string (nullable = true)\n",
            " |-- transaction_date: string (nullable = true)\n",
            " |-- fraude: integer (nullable = true)\n",
            " |-- remetente_nome: string (nullable = true)\n",
            " |-- remetente_banco: string (nullable = true)\n",
            " |-- remetente_tipo: string (nullable = true)\n",
            " |-- destinatario_nome: string (nullable = true)\n",
            " |-- destinatario_banco: string (nullable = true)\n",
            " |-- destinatario_tipo: string (nullable = true)\n",
            "\n",
            "+------------+------------------+---------+-------------+-------------------+------+------------------+---------------+--------------+--------------------+------------------+-----------------+\n",
            "|id_transacao|             valor|chave_pix|    categoria|   transaction_date|fraude|    remetente_nome|remetente_banco|remetente_tipo|   destinatario_nome|destinatario_banco|destinatario_tipo|\n",
            "+------------+------------------+---------+-------------+-------------------+------+------------------+---------------+--------------+--------------------+------------------+-----------------+\n",
            "|        1000|            588.08|aleatoria|       outros|2021-07-16 05:00:55|     0|Jonathan Gonsalves|            BTG|            PF|         Calebe Melo|             Caixa|               PF|\n",
            "|        1001|           80682.5|  celular|transferencia|2022-04-20 12:34:01|     1|Jonathan Gonsalves|            BTG|            PF|  Davi Lucas Pereira|             Caixa|               PJ|\n",
            "|        1002|             549.9|      cpf|        lazer|2022-07-10 16:51:34|     0|Jonathan Gonsalves|            BTG|            PF|      Sabrina Castro|            Nubank|               PF|\n",
            "|        1003|             90.83|aleatoria|   transporte|2022-10-20 10:57:36|     0|Jonathan Gonsalves|            BTG|            PF|Francisco da Conc...|            Nubank|               PJ|\n",
            "|        1004|13272.619999999999|    email|transferencia|2021-04-06 20:26:51|     0|Jonathan Gonsalves|            BTG|            PF|   Isabelly Ferreira|               BTG|               PJ|\n",
            "|        1005|           9347.58|aleatoria|        saude|2022-07-24 15:22:27|     0|Jonathan Gonsalves|            BTG|            PF|Srta. Maria da Cunha|              Itau|               PJ|\n",
            "|        1006|           7836.76|      cpf|    presentes|2022-10-05 19:20:24|     0|Jonathan Gonsalves|            BTG|            PF|     Catarina Duarte|                C6|               PF|\n",
            "|        1007|           3883.62|      cpf|    vestuario|2021-04-24 17:36:34|     0|Jonathan Gonsalves|            BTG|            PF|       Vitor Correia|                XP|               PJ|\n",
            "|        1008|               4.0|aleatoria|        saude|2021-11-16 21:46:47|     0|Jonathan Gonsalves|            BTG|            PF|         Theo Novaes|                C6|               PJ|\n",
            "|        1009|              24.3|      cpf|transferencia|2021-07-26 02:08:49|     0|Jonathan Gonsalves|            BTG|            PF|     Isabel Caldeira|                XP|               PJ|\n",
            "|        1010|           87555.3|aleatoria|transferencia|2022-03-14 15:34:45|     1|Jonathan Gonsalves|            BTG|            PF|Sr. Henrique Cardoso|            Nubank|               PF|\n",
            "|        1011|          21345.91|      cpf|transferencia|2021-10-31 04:31:51|     1|Jonathan Gonsalves|            BTG|            PF|   Felipe Cavalcanti|            Nubank|               PJ|\n",
            "|        1012|          73605.85|  celular|transferencia|2021-04-30 19:19:56|     1|Jonathan Gonsalves|            BTG|            PF|     Dr. Davi da Luz|          Bradesco|               PJ|\n",
            "|        1013|             93.53|      cpf|  alimentacao|2023-01-13 13:39:57|     0|Jonathan Gonsalves|            BTG|            PF|    Stephany Cardoso|                C6|               PJ|\n",
            "|        1014|            564.11|aleatoria|    vestuario|2022-05-27 23:06:08|     0|Jonathan Gonsalves|            BTG|            PF|   Sra. Julia Araujo|              Itau|               PJ|\n",
            "|        1015|              3.59|    email|        saude|2021-10-06 21:19:58|     0|Jonathan Gonsalves|            BTG|            PF|     Carolina Farias|            Nubank|               PJ|\n",
            "|        1016|          19164.89|    email|  alimentacao|2022-03-06 17:59:43|     0|Jonathan Gonsalves|            BTG|            PF|   Isabelly da Costa|            Nubank|               PJ|\n",
            "|        1017|             68.45|aleatoria|    vestuario|2022-04-01 18:17:40|     0|Jonathan Gonsalves|            BTG|            PF|Joao Miguel Silveira|                C6|               PJ|\n",
            "|        1018|            941.25|  celular|    vestuario|2022-05-23 00:28:13|     0|Jonathan Gonsalves|            BTG|            PF|       Matheus Moura|                C6|               PF|\n",
            "|        1019|27009.910000000003|    email|transferencia|2021-08-04 23:22:37|     1|Jonathan Gonsalves|            BTG|            PF| Gabrielly Goncalves|              Itau|               PJ|\n",
            "+------------+------------------+---------+-------------+-------------------+------+------------------+---------------+--------------+--------------------+------------------+-----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_flatten.printSchema()\n",
        "\n",
        "df_flatten.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjAqQcUmjJl_",
        "outputId": "bc76768d-8696-4aeb-f97d-ec0f294b9a5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+-----------------+------------------+---------+-----------+-------------------+------------------+------------------+---------------+--------------+-----------------+------------------+-----------------+\n",
            "|summary|     id_transacao|             valor|chave_pix|  categoria|   transaction_date|            fraude|    remetente_nome|remetente_banco|remetente_tipo|destinatario_nome|destinatario_banco|destinatario_tipo|\n",
            "+-------+-----------------+------------------+---------+-----------+-------------------+------------------+------------------+---------------+--------------+-----------------+------------------+-----------------+\n",
            "|  count|           100000|            100000|   100000|     100000|             100000|            100000|            100000|         100000|        100000|           100000|            100000|           100000|\n",
            "|   mean|          50999.5|10303.358732200059|     NULL|       NULL|               NULL|           0.15367|              NULL|           NULL|          NULL|             NULL|              NULL|             NULL|\n",
            "| stddev|28867.65779668774| 20874.99768875586|     NULL|       NULL|               NULL|0.3606339302787737|              NULL|           NULL|          NULL|             NULL|              NULL|             NULL|\n",
            "|    min|             1000|               0.0|aleatoria|alimentacao|2021-01-14 15:37:45|                 0|Jonathan Gonsalves|            BTG|            PF|   Agatha Almeida|               BTG|               PF|\n",
            "|    max|           100999|          89996.33|    email|  vestuario|2023-01-15 02:51:10|                 1|Jonathan Gonsalves|            BTG|            PF|   Yuri das Neves|                XP|               PJ|\n",
            "+-------+-----------------+------------------+---------+-----------+-------------------+------------------+------------------+---------------+--------------+-----------------+------------------+-----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_flatten.describe().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnOJ5pwHARTD"
      },
      "source": [
        "# Modelagem\n",
        "Aqui você encontrará utilidade para os dados levantados.\n",
        "\n",
        "Aqui será onde teremos insights e, a partir desses, novos conhecimentos sobre o business (se tudo até aqui foi feito corretamente).\n",
        "\n",
        "\n",
        "- Para qual banco esse cliente mais transfere?\n",
        "- Qual é a média de transferências por período que esse cliente faz?\n",
        "- Baseando-se no valor das transferências, poderia dar um aumento de crédito?\n",
        "- Para o que esse cliente mais usa as transferências?\n",
        "- Executar um algoritmo de machine learning que identifique possíveis transações com fraude.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "an6g_DOClI6Y",
        "outputId": "2e2c12d0-2738-4b69-88e7-abbc6d311c63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------+-----+\n",
            "|destinatario_banco|count|\n",
            "+------------------+-----+\n",
            "|          Bradesco|14187|\n",
            "|                C6|14204|\n",
            "|             Caixa|14240|\n",
            "|              Itau|14281|\n",
            "|            Nubank|14297|\n",
            "|               BTG|14390|\n",
            "|                XP|14401|\n",
            "+------------------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_flatten.groupBy('destinatario_banco').count().orderBy('count').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXldp-9zlTBM",
        "outputId": "01f83bb1-f80b-46b1-8646-e27f43141a6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+------------------+-----+\n",
            "|ano_mes|destinatario_banco|count|\n",
            "+-------+------------------+-----+\n",
            "|2023-01|              Itau|  267|\n",
            "|2023-01|             Caixa|  277|\n",
            "|2023-01|                XP|  277|\n",
            "|2023-01|          Bradesco|  280|\n",
            "|2023-01|            Nubank|  290|\n",
            "|2023-01|                C6|  290|\n",
            "|2023-01|               BTG|  278|\n",
            "|2022-12|                XP|  615|\n",
            "|2022-12|               BTG|  603|\n",
            "|2022-12|                C6|  576|\n",
            "|2022-12|          Bradesco|  575|\n",
            "|2022-12|            Nubank|  602|\n",
            "|2022-12|              Itau|  633|\n",
            "|2022-12|             Caixa|  616|\n",
            "|2022-11|          Bradesco|  579|\n",
            "|2022-11|               BTG|  580|\n",
            "|2022-11|              Itau|  614|\n",
            "|2022-11|            Nubank|  620|\n",
            "|2022-11|             Caixa|  543|\n",
            "|2022-11|                C6|  561|\n",
            "+-------+------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import date_format\n",
        "\n",
        "df_flatten.groupBy(\n",
        "    date_format(col('transaction_date'), 'yyyy-MM').alias('ano_mes'),\n",
        "    'destinatario_banco'\n",
        ").count().orderBy(col('ano_mes').desc()).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQvbvJrFlj1D",
        "outputId": "4c546947-4171-47dc-ff8d-c207565a929d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------+------------------+\n",
            "|destinatario_banco|        avg(valor)|\n",
            "+------------------+------------------+\n",
            "|               BTG|10122.299803335622|\n",
            "|              Itau|10230.876305580874|\n",
            "|             Caixa|10254.864015449395|\n",
            "|                C6|10309.499774711307|\n",
            "|            Nubank|10316.475401133126|\n",
            "|                XP|10328.071572113045|\n",
            "|          Bradesco| 10564.19458870794|\n",
            "+------------------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_flatten.groupBy(\n",
        "    'destinatario_banco'\n",
        ").avg('valor').orderBy('avg(valor)').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10WQvkeNlw3n",
        "outputId": "f8aa89bb-cb60-4ea4-9fa3-abd2c91271e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+------------------+-------------+-----+\n",
            "|ano_mes|destinatario_banco|    categoria|count|\n",
            "+-------+------------------+-------------+-----+\n",
            "|2021-01|             Caixa|     educacao|   24|\n",
            "|2021-01|              Itau|        saude|   40|\n",
            "|2021-01|            Nubank|        saude|   28|\n",
            "|2021-01|                XP|    presentes|   38|\n",
            "|2021-01|               BTG|   transporte|   29|\n",
            "|2021-01|               BTG|  alimentacao|   31|\n",
            "|2021-01|              Itau|transferencia|   75|\n",
            "|2021-01|                XP|       outros|   34|\n",
            "|2021-01|                XP|   transporte|   26|\n",
            "|2021-01|          Bradesco|transferencia|   96|\n",
            "|2021-01|              Itau|    vestuario|   28|\n",
            "|2021-01|               BTG|        lazer|   34|\n",
            "|2021-01|          Bradesco|   transporte|   40|\n",
            "|2021-01|                XP|transferencia|   89|\n",
            "|2021-01|                C6|  alimentacao|   38|\n",
            "|2021-01|             Caixa|        lazer|   37|\n",
            "|2021-01|          Bradesco|       outros|   25|\n",
            "|2021-01|            Nubank|    presentes|   33|\n",
            "|2021-01|                C6|transferencia|   87|\n",
            "|2021-01|             Caixa|       outros|   38|\n",
            "+-------+------------------+-------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_flatten.groupBy(\n",
        "    date_format(col('transaction_date'), 'yyyy-MM').alias('ano_mes'),\n",
        "    'destinatario_banco',\n",
        "    'categoria'\n",
        ").count().orderBy('ano_mes').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xi3BMPWl-FP",
        "outputId": "ab54303c-1b70-4731-9a6f-ccd2197eb637"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+-------------+-------------+\n",
            "| ano|    categoria|        valor|\n",
            "+----+-------------+-------------+\n",
            "|2023|    presentes|   362584.450|\n",
            "|2023|  alimentacao|   392078.310|\n",
            "|2023|        saude|   400683.640|\n",
            "|2023|   transporte|   427790.540|\n",
            "|2023|     educacao|   432305.660|\n",
            "|2023|    vestuario|   459528.700|\n",
            "|2023|        lazer|   469671.410|\n",
            "|2023|       outros|   501308.340|\n",
            "|2021|   transporte|  9497893.470|\n",
            "|2021|        lazer|  9622503.140|\n",
            "|2021|       outros|  9737076.380|\n",
            "|2022|        saude| 10048245.070|\n",
            "|2021|     educacao| 10106095.070|\n",
            "|2021|  alimentacao| 10237928.250|\n",
            "|2022|        lazer| 10295747.630|\n",
            "|2022|    presentes| 10311585.740|\n",
            "|2022|     educacao| 10367124.440|\n",
            "|2021|        saude| 10384662.820|\n",
            "|2021|    vestuario| 10412651.930|\n",
            "|2021|    presentes| 10496042.020|\n",
            "|2022|  alimentacao| 10545783.230|\n",
            "|2022|   transporte| 10553408.290|\n",
            "|2022|       outros| 10566645.100|\n",
            "|2022|    vestuario| 10696006.480|\n",
            "|2023|transferencia| 16148682.620|\n",
            "|2021|transferencia|416670741.700|\n",
            "|2022|transferencia|430191098.790|\n",
            "+----+-------------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# soma de quanto ele gasta em cada categoria por mês\n",
        "from pyspark.sql.functions import date_format\n",
        "\n",
        "df_flatten.groupBy(\n",
        "    date_format(col('transaction_date'), 'yyyy').alias('ano'),\n",
        "    'categoria'\n",
        ").sum('valor').select('ano', 'categoria', col('sum(valor)').cast(DecimalType(38, 3)).alias('valor')).orderBy('valor').show(30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rxc4J-91mN_c",
        "outputId": "33c151cb-2eb2-43bb-8edf-a78eee055a0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+------------------+\n",
            "| ano|               avg|\n",
            "+----+------------------+\n",
            "|2021| 51204.03597092333|\n",
            "|2022|50823.494263441935|\n",
            "|2023| 50481.67993874426|\n",
            "+----+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# média de transferencia por período\n",
        "\n",
        "from pyspark.sql.functions import date_format\n",
        "\n",
        "df_flatten.groupBy(\n",
        "    date_format(col('transaction_date'), 'yyyy').alias('ano')\n",
        ").avg('id_transacao').select('ano',col('avg(id_transacao)').alias('avg')).orderBy('ano').show(30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXiPdGqCmZic",
        "outputId": "4409323e-a67a-472f-94ee-0e5b78feccce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+-----+\n",
            "|fraude|count|\n",
            "+------+-----+\n",
            "|     1|15367|\n",
            "|     0|84633|\n",
            "+------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_flatten.groupBy('fraude').count().show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64JPbTfJmhu5",
        "outputId": "e369c9a4-1acf-4178-cbc2-a2690b9a82e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+------+-----+\n",
            "|    categoria|fraude|count|\n",
            "+-------------+------+-----+\n",
            "|       outros|     0| 9377|\n",
            "|     educacao|     0| 9460|\n",
            "|transferencia|     0| 9377|\n",
            "|transferencia|     1|15367|\n",
            "|    presentes|     0| 9254|\n",
            "|        saude|     0| 9476|\n",
            "|        lazer|     0| 9464|\n",
            "|   transporte|     0| 9174|\n",
            "|    vestuario|     0| 9503|\n",
            "|  alimentacao|     0| 9548|\n",
            "+-------------+------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_flatten.groupBy('categoria', 'fraude').count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFX6LJmlmykC",
        "outputId": "3b8722c8-b14f-4b7a-f864-245297d65c1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+------+-----+\n",
            "|    categoria|fraude|count|\n",
            "+-------------+------+-----+\n",
            "|transferencia|     0| 9377|\n",
            "|transferencia|     1|15367|\n",
            "+-------------+------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_flatten.filter(\n",
        "    col('categoria') == 'transferencia'\n",
        ").groupBy('categoria', 'fraude').count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tt-s3NCm1rE",
        "outputId": "ba1c2ac4-9b76-460f-c98f-6e24b7b5184d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+-----+\n",
            "|range|count|\n",
            "+-----+-----+\n",
            "|89000|  222|\n",
            "|88000|  208|\n",
            "|87000|  230|\n",
            "|86000|  203|\n",
            "|85000|  205|\n",
            "|84000|  245|\n",
            "|83000|  206|\n",
            "|82000|  206|\n",
            "|81000|  214|\n",
            "|80000|  213|\n",
            "|79000|  205|\n",
            "|78000|  230|\n",
            "|77000|  237|\n",
            "|76000|  232|\n",
            "|75000|  190|\n",
            "|74000|  207|\n",
            "|73000|  237|\n",
            "|72000|  234|\n",
            "|71000|  234|\n",
            "|70000|  222|\n",
            "+-----+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import floor\n",
        "\n",
        "df_flatten.filter(col('fraude') == 1).withColumn(\n",
        "    \"range\", \n",
        "    floor(col(\"valor\")/1000)*1000\n",
        ").groupBy('range').count().orderBy(col('range').desc()).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7LMZG5anKWO",
        "outputId": "74698373-f472-49ba-e6a1-e2ea3ea2b36a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+----------+\n",
            "|max(range)|min(range)|\n",
            "+----------+----------+\n",
            "|     89000|     19000|\n",
            "+----------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import floor, max, min\n",
        "\n",
        "df_flatten.filter(\n",
        "    col('fraude') == 1\n",
        ").withColumn(\n",
        "    \"range\", floor(col(\"valor\")/1000)*1000\n",
        ").select(max('range'), min('range')).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIVX81NQzsoZ"
      },
      "source": [
        "## Modelo de Predição de Fraudes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "01Hi7z2jn2j4"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, udf\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['id_transacao',\n",
              " 'valor',\n",
              " 'chave_pix',\n",
              " 'categoria',\n",
              " 'transaction_date',\n",
              " 'fraude',\n",
              " 'remetente_nome',\n",
              " 'remetente_banco',\n",
              " 'remetente_tipo',\n",
              " 'destinatario_nome',\n",
              " 'destinatario_banco',\n",
              " 'destinatario_tipo']"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "2y-IDF9toFdL"
      },
      "outputs": [],
      "source": [
        "df = df_flatten.drop('remetente', 'id')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "aDnzz3tUoC1I"
      },
      "outputs": [],
      "source": [
        "indexer = StringIndexer(\n",
        "    inputCols=[\n",
        "        \"destinatario_nome\", \n",
        "        \"destinatario_banco\",\n",
        "        \"destinatario_tipo\",\n",
        "        \"categoria\",\n",
        "        \"chave_pix\"\n",
        "    ], \n",
        "    outputCols=[\n",
        "        \"destinatario_nome_index\", \n",
        "        \"destinatario_banco_index\",\n",
        "        \"destinatario_tipo_index\",\n",
        "        \"categoria_index\",\n",
        "        \"chave_pix_index\"\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khgbk4JVoY5T",
        "outputId": "9bdc89a2-4c6b-4342-a49b-32689fac7385"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+------------------+---------+-------------+-------------------+------+------------------+---------------+--------------+--------------------+------------------+-----------------+-----------------------+------------------------+-----------------------+---------------+---------------+\n",
            "|id_transacao|             valor|chave_pix|    categoria|   transaction_date|fraude|    remetente_nome|remetente_banco|remetente_tipo|   destinatario_nome|destinatario_banco|destinatario_tipo|destinatario_nome_index|destinatario_banco_index|destinatario_tipo_index|categoria_index|chave_pix_index|\n",
            "+------------+------------------+---------+-------------+-------------------+------+------------------+---------------+--------------+--------------------+------------------+-----------------+-----------------------+------------------------+-----------------------+---------------+---------------+\n",
            "|        1000|            588.08|aleatoria|       outros|2021-07-16 05:00:55|     0|Jonathan Gonsalves|            BTG|            PF|         Calebe Melo|             Caixa|               PF|                12045.0|                     4.0|                    1.0|            6.0|            3.0|\n",
            "|        1001|           80682.5|  celular|transferencia|2022-04-20 12:34:01|     1|Jonathan Gonsalves|            BTG|            PF|  Davi Lucas Pereira|             Caixa|               PJ|                  259.0|                     4.0|                    0.0|            0.0|            2.0|\n",
            "|        1002|             549.9|      cpf|        lazer|2022-07-10 16:51:34|     0|Jonathan Gonsalves|            BTG|            PF|      Sabrina Castro|            Nubank|               PF|                  132.0|                     2.0|                    1.0|            4.0|            1.0|\n",
            "|        1003|             90.83|aleatoria|   transporte|2022-10-20 10:57:36|     0|Jonathan Gonsalves|            BTG|            PF|Francisco da Conc...|            Nubank|               PJ|                10475.0|                     2.0|                    0.0|            8.0|            3.0|\n",
            "|        1004|13272.619999999999|    email|transferencia|2021-04-06 20:26:51|     0|Jonathan Gonsalves|            BTG|            PF|   Isabelly Ferreira|               BTG|               PJ|                 4159.0|                     1.0|                    0.0|            0.0|            0.0|\n",
            "|        1005|           9347.58|aleatoria|        saude|2022-07-24 15:22:27|     0|Jonathan Gonsalves|            BTG|            PF|Srta. Maria da Cunha|              Itau|               PJ|                26853.0|                     3.0|                    0.0|            3.0|            3.0|\n",
            "|        1006|           7836.76|      cpf|    presentes|2022-10-05 19:20:24|     0|Jonathan Gonsalves|            BTG|            PF|     Catarina Duarte|                C6|               PF|                 5578.0|                     5.0|                    1.0|            7.0|            1.0|\n",
            "|        1007|           3883.62|      cpf|    vestuario|2021-04-24 17:36:34|     0|Jonathan Gonsalves|            BTG|            PF|       Vitor Correia|                XP|               PJ|                13528.0|                     0.0|                    0.0|            2.0|            1.0|\n",
            "|        1008|               4.0|aleatoria|        saude|2021-11-16 21:46:47|     0|Jonathan Gonsalves|            BTG|            PF|         Theo Novaes|                C6|               PJ|                 1141.0|                     5.0|                    0.0|            3.0|            3.0|\n",
            "|        1009|              24.3|      cpf|transferencia|2021-07-26 02:08:49|     0|Jonathan Gonsalves|            BTG|            PF|     Isabel Caldeira|                XP|               PJ|                 8369.0|                     0.0|                    0.0|            0.0|            1.0|\n",
            "|        1010|           87555.3|aleatoria|transferencia|2022-03-14 15:34:45|     1|Jonathan Gonsalves|            BTG|            PF|Sr. Henrique Cardoso|            Nubank|               PF|                22115.0|                     2.0|                    1.0|            0.0|            3.0|\n",
            "|        1011|          21345.91|      cpf|transferencia|2021-10-31 04:31:51|     1|Jonathan Gonsalves|            BTG|            PF|   Felipe Cavalcanti|            Nubank|               PJ|                 5897.0|                     2.0|                    0.0|            0.0|            1.0|\n",
            "|        1012|          73605.85|  celular|transferencia|2021-04-30 19:19:56|     1|Jonathan Gonsalves|            BTG|            PF|     Dr. Davi da Luz|          Bradesco|               PJ|                12212.0|                     6.0|                    0.0|            0.0|            2.0|\n",
            "|        1013|             93.53|      cpf|  alimentacao|2023-01-13 13:39:57|     0|Jonathan Gonsalves|            BTG|            PF|    Stephany Cardoso|                C6|               PJ|                   10.0|                     5.0|                    0.0|            1.0|            1.0|\n",
            "|        1014|            564.11|aleatoria|    vestuario|2022-05-27 23:06:08|     0|Jonathan Gonsalves|            BTG|            PF|   Sra. Julia Araujo|              Itau|               PJ|                24413.0|                     3.0|                    0.0|            2.0|            3.0|\n",
            "|        1015|              3.59|    email|        saude|2021-10-06 21:19:58|     0|Jonathan Gonsalves|            BTG|            PF|     Carolina Farias|            Nubank|               PJ|                10121.0|                     2.0|                    0.0|            3.0|            0.0|\n",
            "|        1016|          19164.89|    email|  alimentacao|2022-03-06 17:59:43|     0|Jonathan Gonsalves|            BTG|            PF|   Isabelly da Costa|            Nubank|               PJ|                 6122.0|                     2.0|                    0.0|            1.0|            0.0|\n",
            "|        1017|             68.45|aleatoria|    vestuario|2022-04-01 18:17:40|     0|Jonathan Gonsalves|            BTG|            PF|Joao Miguel Silveira|                C6|               PJ|                 6214.0|                     5.0|                    0.0|            2.0|            3.0|\n",
            "|        1018|            941.25|  celular|    vestuario|2022-05-23 00:28:13|     0|Jonathan Gonsalves|            BTG|            PF|       Matheus Moura|                C6|               PF|                 1829.0|                     5.0|                    1.0|            2.0|            2.0|\n",
            "|        1019|27009.910000000003|    email|transferencia|2021-08-04 23:22:37|     1|Jonathan Gonsalves|            BTG|            PF| Gabrielly Goncalves|              Itau|               PJ|                 5973.0|                     3.0|                    0.0|            0.0|            0.0|\n",
            "+------------+------------------+---------+-------------+-------------------+------+------------------+---------------+--------------+--------------------+------------------+-----------------+-----------------------+------------------------+-----------------------+---------------+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_index = indexer.fit(df).transform(df)\n",
        "df_index.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "0QxnRptsohR1"
      },
      "outputs": [],
      "source": [
        "cols_para_filtrar = [\n",
        "  \"valor\",\n",
        "  \"transaction_date\",\n",
        "  \"destinatario_nome_index\", \n",
        "  \"destinatario_banco_index\",\n",
        "  \"destinatario_tipo_index\",\n",
        "  \"chave_pix_index\",\n",
        "  \"categoria_index\",\n",
        "  \"fraude\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "YsD-Xamboo7F"
      },
      "outputs": [],
      "source": [
        "is_fraud = df_index.select(cols_para_filtrar).filter(\"fraude == 1\")\n",
        "no_fraud = df_index.select(cols_para_filtrar).filter(\"fraude == 0\")\n",
        "\n",
        "\n",
        "no_fraud = no_fraud.sample(False, 0.01, seed = 123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXSAOrpuox1J",
        "outputId": "79b8fa9b-ace8-4715-d014-dad858254b75"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "16202"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_concat = no_fraud.union(is_fraud)\n",
        "df = df_concat.sort(\"transaction_date\")\n",
        "df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WL6_PCsJo3GQ",
        "outputId": "f6a6d814-9e37-4d7e-d03e-3e14706829a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train = 11278  test = 4924\n"
          ]
        }
      ],
      "source": [
        "train, test = df.randomSplit([0.7, 0.3], seed = 123)\n",
        "print(\"train =\", train.count(), \" test =\", test.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "fHo4HW1ho6rf"
      },
      "outputs": [],
      "source": [
        "is_fraud = udf(lambda fraud: 1.0 if fraud > 0 else 0.0, DoubleType())\n",
        "train = train.withColumn(\"is_fraud\", is_fraud(train.fraude))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "6GwcMRCHpDL9"
      },
      "outputs": [
        {
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o312.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 68.0 failed 1 times, most recent failure: Lost task 0.0 in stage 68.0 (TID 273) (the-machine executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:54)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.sql.execution.SQLExecutionRDD.$anonfun$compute$1(SQLExecutionRDD.scala:52)\r\n\tat org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:158)\r\n\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 41 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2493)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1202)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1196)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$2(RDD.scala:1289)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1256)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1242)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1242)\r\n\tat org.apache.spark.ml.stat.Summarizer$.getClassificationSummarizers(Summarizer.scala:233)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.$anonfun$train$1(LogisticRegression.scala:517)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:497)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:287)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:54)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.sql.execution.SQLExecutionRDD.$anonfun$compute$1(SQLExecutionRDD.scala:52)\r\n\tat org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:158)\r\n\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 41 more\r\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[32], line 17\u001b[0m\n\u001b[0;32m      9\u001b[0m lr \u001b[38;5;241m=\u001b[39m LogisticRegression()\u001b[38;5;241m.\u001b[39msetParams(\n\u001b[0;32m     10\u001b[0m     maxIter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100000\u001b[39m,\n\u001b[0;32m     11\u001b[0m     labelCol \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_fraud\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     12\u001b[0m     predictionCol \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# This will train a logistic regression model on the input data and return a \u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# LogisticRegressionModel object which can be used to make predictions on new data.\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mPipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43massembler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\CODES\\analise_pix_spark\\.venv\\Lib\\site-packages\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
            "File \u001b[1;32mc:\\CODES\\analise_pix_spark\\.venv\\Lib\\site-packages\\pyspark\\ml\\pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    132\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mtransform(dataset)\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m indexOfLastEstimator:\n",
            "File \u001b[1;32mc:\\CODES\\analise_pix_spark\\.venv\\Lib\\site-packages\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
            "File \u001b[1;32mc:\\CODES\\analise_pix_spark\\.venv\\Lib\\site-packages\\pyspark\\ml\\wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[1;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
            "File \u001b[1;32mc:\\CODES\\analise_pix_spark\\.venv\\Lib\\site-packages\\pyspark\\ml\\wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\CODES\\analise_pix_spark\\.venv\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[1;32mc:\\CODES\\analise_pix_spark\\.venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
            "File \u001b[1;32mc:\\CODES\\analise_pix_spark\\.venv\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
            "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o312.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 68.0 failed 1 times, most recent failure: Lost task 0.0 in stage 68.0 (TID 273) (the-machine executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:54)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.sql.execution.SQLExecutionRDD.$anonfun$compute$1(SQLExecutionRDD.scala:52)\r\n\tat org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:158)\r\n\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 41 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2493)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1202)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1196)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$2(RDD.scala:1289)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1256)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1242)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1242)\r\n\tat org.apache.spark.ml.stat.Summarizer$.getClassificationSummarizers(Summarizer.scala:233)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.$anonfun$train$1(LogisticRegression.scala:517)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:497)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:287)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:54)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.sql.execution.SQLExecutionRDD.$anonfun$compute$1(SQLExecutionRDD.scala:52)\r\n\tat org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:158)\r\n\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 41 more\r\n"
          ]
        }
      ],
      "source": [
        "# Create the feature vectors.\n",
        "# VectorAssembler is a transformer that combines a given list of columns into a single vector column.\n",
        "spark.conf.set(\"spark.network.timeout\", \"600s\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lX71BSspP8B",
        "outputId": "740b8ba9-e22f-476e-aa6e-c0ca54898591"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+---+----+\n",
            "|is_fraud_prediction|0.0| 1.0|\n",
            "+-------------------+---+----+\n",
            "|                1.0|  0|4660|\n",
            "|                0.0|262|   2|\n",
            "+-------------------+---+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "predicted = model.transform(test)\n",
        "\n",
        "predicted = predicted.withColumn(\"is_fraud\", is_fraud(predicted.fraude))\n",
        "predicted.crosstab(\"is_fraud\", \"prediction\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zHoQJWtATfX"
      },
      "source": [
        "# Avaliação do Modelo\n",
        "Será que seu modelo atinge todas as necessidades que foram definidas inicialmente? (e.g. pessoa em cima da bicicleta muda o resultado final)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOTAB1JSAVPi"
      },
      "source": [
        "# Deployment\n",
        "Apresente o relatório com os resultados obtidos.\n",
        "\n",
        "Foi identificado que o cliente Jonathan Gonsalve tem uma alta taxa de transaferências pix mensal. Através de análise realizadas foi possível perceber que a maior categoria de transação é a transferência bancária.\n",
        "\n",
        "Também foi possível notar que o segundo banco que o cliente mais transaciona é o banco BTG, porém, o mesmo possui o menor valor transacionado, o que indica que o cliente faz muitas transações de menor valor para esse banco. \n",
        "\n",
        "Também foi possível verificar que há um alto índice de tentativas de fraude na conta desse cliente, sendo que todas as tentativas de fraude foram com valores acima de R$19.999,00 e com a categoria de transferência. Por isso, foi criado um algoritimo de machine learning que identifica esses tipos de transações que contém fraude. \n",
        "\n",
        "\n",
        "Conclui-se que há uma alta tentativa de transações com fraude e uma ação possível seria diminuir o limite máximo de transferência de pix do cliente.Possivelmente o cliente esteja usando essa conta PF com propósitos de PJ, devido a alta taxa de transferências s altos valores.  "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
